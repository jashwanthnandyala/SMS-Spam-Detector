# -*- coding: utf-8 -*-
"""sms-spam-detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VQFMhUOKNEyxVbPXTtxfGei9sro71Oo8
"""

import numpy as np
import pandas as pd
df=pd.read_csv("/content/spam.csv",encoding="latin-1")

df.head()

"""Data Cleaning"""

df.info()

df.drop(columns=["Unnamed: 2","Unnamed: 3","Unnamed: 4"],inplace=True)

df.head()

df.rename(columns={"v1":"target","v2":"text"},inplace=True)

from sklearn.preprocessing import LabelEncoder

le=LabelEncoder()

df["target"]=le.fit_transform(df["target"])

df.isnull().sum()

df.duplicated().sum()

df=df.drop_duplicates(keep="first")

df.shape

"""EDA"""

import nltk

df['num_chars']=df["text"].apply(len)

nltk.download('punkt_tab')
df['num_words']=df['text'].apply(lambda x: len(nltk.word_tokenize(x)))

df['num_sentence']=df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))

df.head()

!pip install ydata-profiling

from ydata_profiling import ProfileReport

prof=ProfileReport(df)
prof.to_file(output_file='spam.html')

df[["num_chars","num_words","num_sentence"]].describe()

df[df["target"]==0][["num_chars","num_words","num_sentence"]].describe()

df[df["target"]==1][["num_chars","num_words","num_sentence"]].describe()

import seaborn as sns
sns.histplot(df[df["target"]==0]['num_chars'],color='blue')
sns.histplot(df[df["target"]==1]['num_chars'],color='red')



df[["num_chars","num_words","num_sentence","target"]].corr()

# target is most correlated on num_chars

"""Data Preprocessing"""



import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer
ps=PorterStemmer()
nltk.download('stopwords')
nltk.download('punkt')

def text_transform(text):
  text=text.lower()
  text=nltk.word_tokenize(text)

  y=[]
  for i in text:
    if i.isalnum():
      y.append(i)
  text=y[:]
  y.clear()

  for i in text:
    if i not in stopwords.words('english') and i not in string.punctuation:
      y.append(i)
  text=y[:]
  y.clear()

  for i in text:
     y.append(ps.stem(i))
  text=y[:]
  y.clear()
  return " ".join(text)

df['transformed']=df['text'].apply(text_transform)

df.head()

from wordcloud import WordCloud
wc=WordCloud(width=500,height=500,min_font_size=10,background_color='white')
spam_wc=wc.generate(df[df['target']==1]['transformed'].str.cat(sep=" "))

import matplotlib.pyplot as plt
plt.imshow(spam_wc)

ham_wc=wc.generate(df[df['target']==0]['transformed'].str.cat(sep=" "))
plt.imshow(ham_wc)

"""Modelling"""

from sklearn.feature_extraction.text import CountVectorizer

cv=CountVectorizer()

x=cv.fit_transform(df['transformed']).toarray()

x

y=df['target'].values

y

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score

gnb = GaussianNB()
mnb = MultinomialNB()
bnb = BernoulliNB()

gnb.fit(x_train, y_train)
y_pred1 = gnb.predict(x_test)

print(accuracy_score(y_test, y_pred1))
print(confusion_matrix(y_test, y_pred1))
print(precision_score(y_test, y_pred1))

mnb.fit(x_train, y_train)
y_pred2 = mnb.predict(x_test)

print(accuracy_score(y_test, y_pred2))
print(confusion_matrix(y_test, y_pred2))
print(precision_score(y_test, y_pred2))

bnb.fit(x_train, y_train)
y_pred3 = bnb.predict(x_test)

print(accuracy_score(y_test, y_pred3))
print(confusion_matrix(y_test, y_pred3))
print(precision_score(y_test, y_pred3))

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
X = tfidf.fit_transform(df['transformed']).toarray()

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score

gnb = GaussianNB()
mnb = MultinomialNB()
bnb = BernoulliNB()

gnb.fit(x_train, y_train)
y_pred2_1 = gnb.predict(x_test)

print(accuracy_score(y_test, y_pred2_1))
print(confusion_matrix(y_test, y_pred2_1))
print(precision_score(y_test, y_pred2_1))

mnb.fit(x_train, y_train)
y_pred2_2 = mnb.predict(x_test)

print(accuracy_score(y_test, y_pred2_2))
print(confusion_matrix(y_test, y_pred2_2))
print(precision_score(y_test, y_pred2_2))

bnb.fit(x_train, y_train)
y_pred2_3 = bnb.predict(x_test)

print(accuracy_score(y_test, y_pred2_3))
print(confusion_matrix(y_test, y_pred2_3))
print(precision_score(y_test, y_pred2_3))

# best is tfid--> mnb
#because of its high precision rather than accuracy as it is imbalnced dataset(ham>spam)

import pickle

pickle.dump(tfidf,open('vectorizer.pkl','wb'))

pickle.dump(mnb,open('spam.pkl','wb'))

